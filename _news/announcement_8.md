---
layout: post
date: 2021-10-05 15:59:00-0400
inline: true
---

Happy to announce our latest breakthrough in Language Modelling using Legendre Memory Units (LMUs)! Our new architecture attains the same accuracy as transformers with 10x fewer tokens. For the same amount of training, our model improves the loss over transformers about as much as transformers improve over LSTMs. Additionally, adding global self-attention complements our architecture and the augmented model improves performance even further.